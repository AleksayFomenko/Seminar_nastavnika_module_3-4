{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%spark.pyspark from pyspark.sql import functions as F\n",
        "users = spark.createDataFrame(\n",
        "[ (\"u1\", \"Berlin\"),\n",
        "(\"u2\", \"Berlin\"),\n",
        "(\"u3\", \"Munich\"),\n",
        "(\"u4\", \"Hamburg\"), ],\n",
        "[\"user_id\", \"city\"] )\n",
        "orders = spark.createDataFrame(\n",
        "[ (\"o1\", \"u1\", \"p1\", 2, 10.0),\n",
        "(\"o2\", \"u1\", \"p2\", 1, 30.0),\n",
        "(\"o3\", \"u2\", \"p1\", 1, 10.0),\n",
        "(\"o4\", \"u2\", \"p3\", 5, 7.0),\n",
        "(\"o5\", \"u3\", \"p2\", 3, 30.0),\n",
        "(\"o6\", \"u3\", \"p3\", 1, 7.0),\n",
        "(\"o7\", \"u4\", \"p1\", 10, 10.0), ],\n",
        "[\"order_id\", \"user_id\", \"product_id\", \"qty\", \"price\"] )\n",
        "products = spark.createDataFrame(\n",
        "[ (\"p1\", \"Ring VOLA\"),\n",
        "(\"p2\", \"Ring POROG\"),\n",
        "(\"p3\", \"Ring TISHINA\"), ],\n",
        "[\"product_id\", \"product_name\"] )\n",
        "users.show() \n",
        "orders.show() \n",
        "products.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![1.png](./pictures/1.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%spark.pyspark\n",
        "orders = orders.withColumn(\"revenue\", F.col(\"qty\")*F.col(\"price\"))\n",
        "orders.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![2.png](./pictures/second.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%spark.pyspark\n",
        "union_df = users.join(orders, \"user_id\")\n",
        "union_df = union_df.join(products, \"product_id\")\n",
        "union_df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![3.png](./pictures/3.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%spark.pyspark\n",
        "orders_cnt = union_df.groupBy(\"city\", \"product_id\", \"product_name\").agg(F.count(\"order_id\").alias(\"orders_cnt\"), F.sum(\"qty\").alias(\"qty_sum\"), F.sum(\"revenue\").alias(\"revenue_sum\"))\n",
        "orders_cnt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![4.png](./pictures/four.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%spark.pyspark\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import row_number, desc\n",
        "w = Window.partitionBy(\"city\").orderBy(desc(\"revenue_sum\"))\n",
        "orders_cnt = orders_cnt.withColumn(\"top_2_products\", row_number().over(w)).filter(F.col(\"top_2_products\")<=2).drop(\"top_2_products\")\n",
        "orders_cnt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![5.png](./pictures/five.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%spark.pyspark\n",
        "hdfs_path = \"/tmp/sandbox_zeppelin/mart_city_top_products/\"\n",
        "orders_cnt.write.mode(\"overwrite\").parquet(hdfs_path)\n",
        "spark.read.parquet(hdfs_path).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![6.png](./pictures/six.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%spark.pyspark\n",
        "s3_path = \"s3a://hadoop2\" + hdfs_path\n",
        "orders_cnt.write.mode(\"overwrite\").parquet(s3_path)\n",
        "spark.read.parquet(s3_path).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![7.png](./pictures/six.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%spark.pyspark\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    },
    "name": "DZ1"
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
